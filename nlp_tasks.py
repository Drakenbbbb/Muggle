# ==============================================================================
# çŸ­è§†é¢‘å†…å®¹æ ‡ç­¾ä¸çƒ­ç‚¹åˆ†æç³»ç»Ÿ (V3.0 é«˜çº§ç‰ˆ)
#
# æ ¸å¿ƒåŠŸèƒ½:
# 1. çƒ­ç‚¹è¶‹åŠ¿æ€»è§ˆï¼šå¤šç»´åº¦åˆ†æçƒ­é—¨è§†é¢‘ã€æ ‡ç­¾ä¸åˆ›ä½œè€…ã€‚
# 2. çˆ†æ¬¾å¯†ç æŒ–æ˜ï¼šä½¿ç”¨Aprioriç®—æ³•æŒ–æ˜é«˜æ”¶ç›Šçš„æ ‡ç­¾ç»„åˆã€‚
# 3. å†…å®¹ç­–ç•¥åˆ†æï¼šå¯¹å¤´éƒ¨åˆ›ä½œè€…è¿›è¡Œé›·è¾¾å›¾å¯¹æ¯”åˆ†æã€‚
# 4. æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆï¼šä¸€é”®å¯¼å‡ºæ‰€æœ‰åˆ†æç»“æœä¸ºWordæ–‡æ¡£ã€‚
# ==============================================================================

import os
import streamlit as st
import pandas as pd
import numpy as np
import ast
import re
import jieba
import lightgbm as lgb
import plotly.express as px
import plotly.graph_objects as go
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
from docx import Document
from docx.shared import Inches
import io
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import matplotlib
import platform

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“ï¼Œè‡ªåŠ¨é€‚é…æ“ä½œç³»ç»Ÿ
def set_chinese_font():
    if platform.system() == 'Windows':
        matplotlib.rcParams['font.sans-serif'] = ['SimHei']  # é»‘ä½“
    elif platform.system() == 'Darwin':
        matplotlib.rcParams['font.sans-serif'] = ['Heiti TC']  # macOS
    else:
        matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']  # Linuxå¸¸ç”¨
    matplotlib.rcParams['axes.unicode_minus'] = False  # è´Ÿå·æ­£å¸¸æ˜¾ç¤º

# ==============================================================================
# 0. é¡µé¢é…ç½® (å¿…é¡»æ˜¯ç¬¬ä¸€ä¸ªstå‘½ä»¤)
# ==============================================================================
st.set_page_config(page_title="Bç«™å†…å®¹å†³ç­–å·¥å…·", layout="wide", initial_sidebar_state="expanded")


# ==============================================================================
# 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†æ¨¡å— (å”¯ä¸€æ­£ç¡®ç‰ˆæœ¬)
# ==============================================================================
@st.cache_data
def load_data(csv_path):
    """
    åŠ è½½å¹¶é¢„å¤„ç†Bç«™çƒ­é—¨è§†é¢‘æ•°æ®ã€‚
    è¿™æ˜¯ç³»ç»Ÿä¸­å”¯ä¸€çš„æ•°æ®åŠ è½½å‡½æ•°ï¼Œç¡®ä¿æ‰€æœ‰æ¨¡å—ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®æºã€‚
    """
    try:
        df = pd.read_csv(csv_path)

        # æ ¸å¿ƒä¿®æ­£ï¼šå°†upload_timeè½¬æ¢ä¸ºcreated_atï¼Œå¹¶å¤„ç†æ½œåœ¨é”™è¯¯
        if 'upload_time' in df.columns:
            # errors='coerce' ä¼šå°†æ— æ³•è§£æçš„æ—¥æœŸå˜ä¸ºNaTï¼ˆNot a Timeï¼‰ï¼Œè€Œä¸æ˜¯æŠ¥é”™
            df['created_at'] = pd.to_datetime(df['upload_time'], errors='coerce')
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è½¬æ¢å¤±è´¥çš„è¡Œ
            if df['created_at'].isnull().any():
                st.warning("éƒ¨åˆ†è¡Œçš„ 'upload_time' æ ¼å¼ä¸æ­£ç¡®ï¼Œå·²è·³è¿‡ã€‚")
                df.dropna(subset=['created_at'], inplace=True) # ç§»é™¤è½¬æ¢å¤±è´¥çš„è¡Œ
        else:
            # å¦‚æœå®Œå…¨æ²¡æœ‰è¯¥åˆ—ï¼Œåˆ™æ— æ³•è¿›è¡Œæ—¶é—´åˆ†æ
            st.error("é”™è¯¯ï¼šæ•°æ®æ–‡ä»¶ä¸­ç¼ºå°‘ 'upload_time' åˆ—ï¼Œæ— æ³•è¿›è¡Œè¶‹åŠ¿åˆ†æã€‚")
            return pd.DataFrame()

        # å®‰å…¨åœ°å°†tagså­—ç¬¦ä¸²è§£æä¸ºlist
        if 'tags' in df.columns:
            df['tags_list'] = df['tags'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith('[') else [])
        else:
            df['tags_list'] = [[] for _ in range(len(df))]

        # æ¸…æ´—å’Œè½¬æ¢æ•°æ®ç±»å‹
        df['title'] = df['title'].apply(lambda x: str(x) if pd.notnull(x) else '')
        df['author'] = df['author'].astype(str)

        # è®¡ç®—äº’åŠ¨é‡ (ç‚¹èµ+è¯„è®º+æŠ•å¸)
        for col in ['likes', 'comments', 'coins']:
            if col not in df.columns:
                df[col] = 0
        df['engagement'] = df['likes'].astype(int) + df['comments'].astype(int) + df['coins'].astype(int)

        # è®¡ç®—äº’åŠ¨ç‡ (äº’åŠ¨é‡/æ’­æ”¾é‡)ï¼Œå¤„ç†åˆ†æ¯ä¸º0çš„æƒ…å†µ
        if 'views' not in df.columns:
            df['views'] = 0
        df['engagement_rate'] = (df['engagement'] / df['views'].replace(0, np.nan)).fillna(0)

        # åˆ›å»ºä¸»æ ‡ç­¾
        df['main_tag'] = df['tags_list'].apply(lambda x: x[0] if x else 'æœªçŸ¥')

        # åˆ›å»ºç”¨äºNLPçš„ç»„åˆæ–‡æœ¬ç‰¹å¾
        df['text_for_nlp'] = df['title'] + ' ' + df['tags_list'].apply(lambda x: ' '.join(x))

        return df

    except FileNotFoundError:
        st.error(f"é”™è¯¯ï¼šæ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·ç¡®è®¤è·¯å¾„ '{csv_path}' æ˜¯å¦æ­£ç¡®ã€‚")
        return pd.DataFrame()
    except Exception as e:
        st.error(f"åŠ è½½æˆ–å¤„ç†æ•°æ®æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{e}")
        return pd.DataFrame()


# ==============================================================================
# 2. æ ¸å¿ƒåˆ†æä¸å»ºæ¨¡æ¨¡å— (å¸¦ç¼“å­˜)
# ==============================================================================
@st.cache_resource  # ç¼“å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
def get_text_classifier(df, min_count=5):
    """è®­ç»ƒå¹¶ç¼“å­˜ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»å™¨ã€‚"""
    tag_counts = df['main_tag'].value_counts()
    valid_tags = tag_counts[tag_counts >= min_count].index
    df_filtered = df[df['main_tag'].isin(valid_tags)].copy()
    X = df_filtered['text_for_nlp']
    y = df_filtered['main_tag']
    vectorizer = TfidfVectorizer(tokenizer=lambda x: jieba.cut(x), max_features=2000)
    X_vec = vectorizer.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.25, random_state=42, stratify=y)
    clf = lgb.LGBMClassifier(random_state=42)
    clf.fit(X_train, y_train)
    return clf, vectorizer, X_test, y_test, valid_tags


@st.cache_resource
def get_hot_predictor(df, top_n=100):
    """è®­ç»ƒå¹¶ç¼“å­˜ä¸€ä¸ªçˆ†æ¬¾æ½œåŠ›é¢„æµ‹å™¨ã€‚"""
    df_copy = df.copy()
    df_copy['is_hot'] = 0
    df_copy.iloc[:top_n, df_copy.columns.get_loc('is_hot')] = 1
    X = df_copy['text_for_nlp']
    y = df_copy['is_hot']
    vectorizer = TfidfVectorizer(tokenizer=lambda x: jieba.cut(x), max_features=2000)
    X_vec = vectorizer.fit_transform(X)
    X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.25, random_state=42, stratify=y)
    clf = lgb.LGBMClassifier(random_state=42)
    clf.fit(X_train, y_train)
    return clf, vectorizer, X_test, y_test


def analyze_top_authors(df, top_n=10):
    """åˆ†æå¤´éƒ¨åˆ›ä½œè€…çš„æ ¸å¿ƒæŒ‡æ ‡ã€‚"""
    author_stats = df.groupby('author').agg(
        video_count=('title', 'count'),
        total_views=('views', 'sum'),
        total_likes=('likes', 'sum'),
        total_engagement=('engagement', 'sum'),
        avg_engagement_rate=('engagement_rate', 'mean')
    ).sort_values('total_views', ascending=False).head(top_n)
    return author_stats


@st.cache_data
def get_association_rules(df, min_support=0.01, min_confidence=0.1):
    """ä½¿ç”¨Aprioriç®—æ³•æŒ–æ˜æ ‡ç­¾å…³è”è§„åˆ™ã€‚"""
    # è¿‡æ»¤æ‰æ²¡æœ‰æ ‡ç­¾çš„è§†é¢‘
    tags_df = df[df['tags_list'].apply(len) > 0]

    # æ•°æ®è½¬æ¢
    te = TransactionEncoder()
    te_ary = te.fit(tags_df['tags_list']).transform(tags_df['tags_list'])
    rules_df = pd.DataFrame(te_ary, columns=te.columns_)

    # æŒ–æ˜é¢‘ç¹é¡¹é›†
    frequent_itemsets = apriori(rules_df, min_support=min_support, use_colnames=True)
    if frequent_itemsets.empty:
        return pd.DataFrame()  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é¢‘ç¹é¡¹é›†ï¼Œè¿”å›ç©ºè¡¨

    # è®¡ç®—å…³è”è§„åˆ™
    rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=min_confidence)
    rules = rules.sort_values(['lift', 'confidence'], ascending=[False, False])
    return rules


# ==============================================================================
# 3. æŠ¥å‘Šç”Ÿæˆæ¨¡å—
# ==============================================================================
def generate_word_report(df, top_authors_stats, top_tags_stats, rules):
    """ç”Ÿæˆå¹¶è¿”å›ä¸€ä¸ªåŒ…å«æ ¸å¿ƒåˆ†æçš„Wordæ–‡æ¡£çš„å†…å­˜å­—èŠ‚æµã€‚"""
    document = Document()
    document.add_heading('Bç«™çƒ­é—¨å†…å®¹è¶‹åŠ¿åˆ†ææŠ¥å‘Š', 0)

    # --- å†…å®¹åˆ›ä½œå»ºè®® ---
    document.add_heading('ä¸€ã€æ ¸å¿ƒå†…å®¹åˆ›ä½œå»ºè®®', level=1)
    document.add_paragraph(generate_insight(df), style='List Bullet')
    document.add_paragraph(f"çƒ­é—¨å†…å®¹ä¸»è¦é›†ä¸­åœ¨'{top_tags_stats.index[0]}'ã€'{top_tags_stats.index[1]}'ç­‰é¢†åŸŸã€‚",
                           style='List Bullet')
    document.add_paragraph(f"å¤´éƒ¨åˆ›ä½œè€…å¦‚'{top_authors_stats.index[0]}'åœ¨æ’­æ”¾é‡ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼Œå…¶å†…å®¹ç­–ç•¥å€¼å¾—å€Ÿé‰´ã€‚",
                           style='List Bullet')
    if not rules.empty:
        top_rule = rules.iloc[0]
        antecedent = ', '.join(list(top_rule['antecedents']))
        consequent = ', '.join(list(top_rule['consequents']))
        document.add_paragraph(
            f"æ•°æ®æ˜¾ç¤ºï¼Œ'{antecedent}'ä¸'{consequent}'çš„ç»„åˆå…·æœ‰å¾ˆé«˜çš„å…³è”æ€§ (æå‡åº¦: {top_rule['lift']:.2f})ï¼Œæ˜¯æ½œåœ¨çš„çˆ†æ¬¾å¯†ç ã€‚",
            style='List Bullet')

    # --- çƒ­é—¨æ ‡ç­¾åˆ†æ ---
    document.add_heading('äºŒã€çƒ­é—¨æ ‡ç­¾åˆ†æ', level=1)
    p = document.add_paragraph('ä¸‹å›¾å±•ç¤ºäº†çƒ­é—¨è§†é¢‘ä¸­å‡ºç°æ¬¡æ•°æœ€å¤šçš„Top 20æ ‡ç­¾ã€‚')
    fig1 = px.bar(top_tags_stats, x=top_tags_stats.index, y='count', title="çƒ­é—¨æ ‡ç­¾Top 20 (æŒ‰å‡ºç°æ¬¡æ•°)",
                  labels={'x': 'æ ‡ç­¾', 'y': 'å‡ºç°æ¬¡æ•°'})
    img_buffer = io.BytesIO()
    fig1.write_image(img_buffer, format="png")
    img_buffer.seek(0)
    document.add_picture(img_buffer, width=Inches(6.0))

    # --- å¤´éƒ¨åˆ›ä½œè€…åˆ†æ ---
    document.add_heading('ä¸‰ã€å¤´éƒ¨åˆ›ä½œè€…åˆ†æ', level=1)
    p = document.add_paragraph('ä¸‹è¡¨å±•ç¤ºäº†æ€»æ’­æ”¾é‡æœ€é«˜çš„Top 10åˆ›ä½œè€…çš„æ ¸å¿ƒæ•°æ®ã€‚')
    # æ·»åŠ è¡¨æ ¼
    table_df = top_authors_stats.reset_index()
    table = document.add_table(rows=1, cols=len(table_df.columns))
    table.style = 'Table Grid'
    hdr_cells = table.rows[0].cells
    for i, col_name in enumerate(table_df.columns):
        hdr_cells[i].text = col_name
    for index, row in table_df.iterrows():
        row_cells = table.add_row().cells
        for i, val in enumerate(row):
            row_cells[i].text = str(val)

    # --- æ€»ç»“ ---
    document.add_heading('å››ã€æ€»ç»“', level=1)
    document.add_paragraph(f"æœ¬æ¬¡åˆ†æåŸºäº {len(df)} æ¡çƒ­é—¨è§†é¢‘æ•°æ®ã€‚æ ¸å¿ƒå‘ç°åŒ…æ‹¬ï¼š")
    document.add_paragraph(f"çƒ­é—¨å†…å®¹ä¸»è¦é›†ä¸­åœ¨'{top_tags_stats.index[0]}'ã€'{top_tags_stats.index[1]}'ç­‰é¢†åŸŸã€‚",
                           style='List Bullet')
    document.add_paragraph(f"å¤´éƒ¨åˆ›ä½œè€…å¦‚'{top_authors_stats.index[0]}'åœ¨æ’­æ”¾é‡ä¸Šå…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚", style='List Bullet')

    # å°†æ–‡æ¡£ä¿å­˜åˆ°å†…å­˜å­—èŠ‚æµ
    doc_buffer = io.BytesIO()
    document.save(doc_buffer)
    doc_buffer.seek(0)
    return doc_buffer


# ==============================================================================
# 4. Streamlit UI é¡µé¢æ¸²æŸ“
# ==============================================================================

# --- é¡µé¢1: çƒ­ç‚¹è¶‹åŠ¿æ€»è§ˆ ---
def page_overview(df):
    st.title("ğŸ“ˆ çƒ­ç‚¹è¶‹åŠ¿æ€»è§ˆ")
    st.markdown("---")

    col1, col2, col3, col4 = st.columns(4)
    col1.metric("è§†é¢‘æ€»æ•°", f"{len(df):,}")
    col2.metric("æ€»æ’­æ”¾é‡", f"{df['views'].sum():,.0f}")
    col3.metric("æ€»ç‚¹èµé‡", f"{df['likes'].sum():,.0f}")
    col4.metric("åˆ›ä½œè€…æ€»æ•°", f"{df['author'].nunique():,}")

    st.markdown("### çƒ­é—¨æ ‡ç­¾åˆ†æ")
    tag_stats = df.explode('tags_list').groupby('tags_list').agg(
        count=('title', 'count'),
        avg_views=('views', 'mean')
    ).sort_values('count', ascending=False)

    top_tags_by_count = tag_stats['count'].head(20)
    fig1 = px.bar(top_tags_by_count, x=top_tags_by_count.index, y=top_tags_by_count.values,
                  title="çƒ­é—¨æ ‡ç­¾Top 20 (æŒ‰å‡ºç°æ¬¡æ•°)", labels={'x': 'æ ‡ç­¾', 'y': 'å‡ºç°æ¬¡æ•°'})
    st.plotly_chart(fig1, use_container_width=True)

    st.markdown("### å¤´éƒ¨åˆ›ä½œè€…åˆ†æ (æŒ‰æ€»æ’­æ”¾é‡)")
    top_authors_stats = analyze_top_authors(df)
    st.dataframe(top_authors_stats.style.format("{:,.0f}"))


# --- é¡µé¢2: çˆ†æ¬¾å¯†ç æŒ–æ˜ (Apriori) ---
def page_association_rules(df):
    st.title("ğŸ”‘ çˆ†æ¬¾å¯†ç æŒ–æ˜ (Aprioriç®—æ³•)")
    st.info("æŒ–æ˜èƒ½å¸¦æ¥æµé‡å¢ç›Šçš„'æ ‡ç­¾é»„é‡‘ç»„åˆ'ã€‚Lift(æå‡åº¦) > 1 è¡¨ç¤ºæ­£å‘å…³è”ã€‚")

    col1, col2 = st.columns([1, 3])
    with col1:
        st.subheader("å‚æ•°è°ƒæ•´")
        min_support = st.slider("æœ€å°æ”¯æŒåº¦ (Min Support)", 0.001, 0.1, 0.01, 0.001, format="%.3f")
        min_confidence = st.slider("æœ€å°ç½®ä¿¡åº¦ (Min Confidence)", 0.05, 0.5, 0.1, 0.05)

    rules = get_association_rules(df, min_support, min_confidence)

    with col2:
        st.subheader("å…³è”è§„åˆ™ç»“æœ")
        if not rules.empty:
            display_rules = rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].copy()
            display_rules['antecedents'] = display_rules['antecedents'].apply(lambda x: ', '.join(list(x)))
            display_rules['consequents'] = display_rules['consequents'].apply(lambda x: ', '.join(list(x)))
            st.dataframe(
                display_rules.head(20).style.format({'support': '{:.3f}', 'confidence': '{:.3f}', 'lift': '{:.2f}'}))
        else:
            st.warning("åœ¨å½“å‰å‚æ•°ä¸‹æœªæ‰¾åˆ°å…³è”è§„åˆ™ï¼Œè¯·å°è¯•é™ä½æœ€å°æ”¯æŒåº¦æˆ–æœ€å°ç½®ä¿¡åº¦ã€‚")


# --- é¡µé¢3: å†…å®¹ç­–ç•¥åˆ†æ ---
def page_strategy_analysis(df):
    st.title("ğŸ¯ å†…å®¹ç­–ç•¥åˆ†æ")

    st.subheader("å¤´éƒ¨åˆ›ä½œè€…å†…å®¹ç­–ç•¥é›·è¾¾å›¾")
    top_authors_stats = analyze_top_authors(df)

    # æ•°æ®å½’ä¸€åŒ–ï¼Œç”¨äºé›·è¾¾å›¾
    scaler = lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() - x.min() != 0 else x / x.max()
    normalized_stats = top_authors_stats.apply(scaler).reset_index()

    fig = go.Figure()
    for index, row in normalized_stats.iterrows():
        fig.add_trace(go.Scatterpolar(
            r=row.drop('author').values,
            theta=row.drop('author').index,
            fill='toself',
            name=row['author']
        ))

    fig.update_layout(
        polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
        showlegend=True,
        title="Top 10åˆ›ä½œè€…æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”"
    )
    st.plotly_chart(fig, use_container_width=True)


# --- æ–°å¢é¡µé¢4: AIæ¨¡å‹ä¸­å¿ƒ ---
def page_models_center(df):
    st.title("ğŸ¤– AIæ¨¡å‹ä¸­å¿ƒ")
    st.info("åœ¨è¿™é‡Œï¼Œä½ å¯ä»¥è¯„ä¼°ç³»ç»Ÿå†…ç½®çš„AIæ¨¡å‹æ€§èƒ½ï¼Œå¹¶è¿›è¡Œäº¤äº’å¼é¢„æµ‹ã€‚")

    # --- å†…å®¹é¢†åŸŸåˆ†ç±»æ¨¡å‹ ---
    with st.container(border=True):
        st.subheader("ğŸ·ï¸ å†…å®¹é¢†åŸŸåˆ†ç±»æ¨¡å‹")
        clf, vectorizer, X_test, y_test, valid_tags = get_text_classifier(df)
        y_pred = clf.predict(X_test)

        st.markdown("**æ¨¡å‹æ€§èƒ½:**")
        col1, col2 = st.columns(2)
        col1.metric("æ¨¡å‹å‡†ç¡®ç‡ (Accuracy)", f"{accuracy_score(y_test, y_pred):.2%}")
        col2.metric("å®å¹³å‡ F1-Score", f"{f1_score(y_test, y_pred, average='macro', zero_division=0):.3f}")
        with st.expander("ç‚¹å‡»æŸ¥çœ‹è¯¦ç»†åˆ†ç±»æŠ¥å‘Š"):
            st.text(classification_report(y_test, y_pred, digits=3, zero_division=0))

        with st.form(key='classifier_form'):
            user_input = st.text_input('è¾“å…¥è§†é¢‘æ ‡é¢˜æˆ–æ ‡ç­¾:', 'ä»‹ç»ä¸€æ¬¾æœ€æ–°çš„AIå·¥å…·')
            submitted = st.form_submit_button('é¢„æµ‹ä¸»æ ‡ç­¾')
            if submitted and user_input:
                pred_tag = clf.predict(vectorizer.transform([user_input]))[0]
                st.success(f'é¢„æµ‹ä¸»æ ‡ç­¾: **{pred_tag}**')

    # --- çˆ†æ¬¾æ½œåŠ›é¢„æµ‹æ¨¡å‹ ---
    with st.container(border=True):
        st.subheader("ğŸš€ çˆ†æ¬¾æ½œåŠ›é¢„æµ‹æ¨¡å‹")
        hot_clf, hot_vectorizer, X_test_hot, y_test_hot = get_hot_predictor(df)
        y_pred_hot = hot_clf.predict(X_test_hot)
        y_prob_hot = hot_clf.predict_proba(X_test_hot)[:, 1]

        st.markdown("**æ¨¡å‹æ€§èƒ½:**")
        col1, col2, col3 = st.columns(3)
        col1.metric("æ¨¡å‹å‡†ç¡®ç‡ (Accuracy)", f"{accuracy_score(y_test_hot, y_pred_hot):.2%}")
        col2.metric("F1-Score", f"{f1_score(y_test_hot, y_pred_hot, zero_division=0):.3f}")
        col3.metric("AUC", f"{roc_auc_score(y_test_hot, y_prob_hot):.3f}")

        with st.form(key='predictor_form'):
            user_input2 = st.text_input('è¾“å…¥ä½ çš„å†…å®¹åˆ›æ„:', 'æŒ‘æˆ˜24å°æ—¶åªåƒä¾¿åˆ©åº—é£Ÿç‰©')
            submitted2 = st.form_submit_button('é¢„æµ‹çˆ†æ¬¾æ½œåŠ›')
            if submitted2 and user_input2:
                prob = hot_clf.predict_proba(hot_vectorizer.transform([user_input2]))[0, 1]
                st.progress(prob)
                st.success(f'**çˆ†æ¬¾æ¦‚ç‡: {prob:.2%}**')


# --- é¡µé¢5: æŠ¥å‘Šç”Ÿæˆä¸ä¸‹è½½ ---
def page_report_generation(df):
    st.title("ğŸ“„ æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆ")
    st.info("ä¸€é”®ç”ŸæˆåŒ…å«æ ¸å¿ƒå›¾è¡¨ä¸æ•°æ®çš„åˆ†ææŠ¥å‘Šã€‚æ³¨æ„ï¼šæŠ¥å‘Šç”Ÿæˆï¼ˆç‰¹åˆ«æ˜¯ç¬¬ä¸€æ¬¡ï¼‰å¯èƒ½éœ€è¦10-20ç§’ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚")

    if st.button("ç”Ÿæˆåˆ†ææŠ¥å‘Š (Word)"):
        with st.spinner("æ­£åœ¨åˆ†ææ•°æ®å¹¶æ¸²æŸ“å›¾è¡¨ï¼Œè¯·ç¨å€™..."):
            tag_stats = df.explode('tags_list').groupby('tags_list').agg(count=('title', 'count')).sort_values('count',
                                                                                                               ascending=False)
            top_authors_stats = analyze_top_authors(df)
            rules = get_association_rules(df)

            report_buffer = generate_word_report(df, top_authors_stats, tag_stats.head(20), rules)

            st.success("æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ï¼ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®ä¸‹è½½ã€‚")
            st.download_button(
                label="ä¸‹è½½WordæŠ¥å‘Š",
                data=report_buffer,
                file_name="Bç«™çƒ­ç‚¹åˆ†ææŠ¥å‘Š.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            )


# --- è¶‹åŠ¿é¢„æµ‹é¡µé¢ ---
def page_trend_forecast(df):
    st.title("ğŸ“Š è¶‹åŠ¿é¢„æµ‹ä¸çƒ­ç‚¹æŒ–æ˜")
    st.info("åŸºäºå†å²æ•°æ®ï¼Œè‡ªåŠ¨è¯†åˆ«è¿‘æœŸé«˜æ½œåŠ›æ ‡ç­¾å’Œå†…å®¹æ–¹å‘ï¼Œè¾…åŠ©é€‰é¢˜å†³ç­–ã€‚")

    if 'created_at' not in df.columns or df['created_at'].isnull().all():
        st.warning("æ•°æ®ä¸­ç¼ºå°‘æœ‰æ•ˆçš„ 'created_at' æ—¶é—´åˆ—ï¼Œæ— æ³•è¿›è¡Œè¶‹åŠ¿åˆ†æã€‚")
        return

    # 1. æ ‡ç­¾çƒ­åº¦è¶‹åŠ¿
    st.subheader("æ ‡ç­¾çƒ­åº¦è¶‹åŠ¿")
    
    # ä¿®æ­£ï¼šå°† 'created_at' è®¾ç½®ä¸ºç´¢å¼•ä»¥è¿›è¡Œæ—¶é—´åºåˆ—åˆ†ç»„
    trend_df = df.set_index('created_at')
    tag_trend = trend_df.explode('tags_list').groupby(['tags_list', pd.Grouper(freq='7D')])['views'].sum().reset_index()
    
    if not tag_trend.empty:
        top_tags = tag_trend.groupby('tags_list')['views'].sum().nlargest(10).index.tolist()
        tag_trend_top = tag_trend[tag_trend['tags_list'].isin(top_tags)]
        fig = px.line(tag_trend_top, x='created_at', y='views', color='tags_list', markers=True, title="è¿‘30å¤©çƒ­é—¨æ ‡ç­¾è¶‹åŠ¿")
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("æ•°æ®é‡ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆæ ‡ç­¾çƒ­åº¦è¶‹åŠ¿å›¾ã€‚")

    # 2. æ½œåŠ›æ ‡ç­¾æ¨èï¼ˆè¿‘7å¤©å¢é€Ÿæœ€å¿«ï¼‰
    st.subheader("é«˜æ½œåŠ›æ ‡ç­¾æ¨è")
    recent = df[df['created_at'] > df['created_at'].max() - pd.Timedelta(days=7)]
    last = df[(df['created_at'] <= df['created_at'].max() - pd.Timedelta(days=7)) & (df['created_at'] > df['created_at'].max() - pd.Timedelta(days=14))]
    
    if not recent.empty and not last.empty:
        recent_tags = recent.explode('tags_list')['tags_list'].value_counts()
        last_tags = last.explode('tags_list')['tags_list'].value_counts()
        growth = (recent_tags - last_tags).sort_values(ascending=False).dropna().head(10)
        st.dataframe(growth.rename('è¿‘7å¤©å‡€å¢é•¿').to_frame())
    else:
        st.info("æ•°æ®æ—¶é—´è·¨åº¦ä¸è¶³14å¤©ï¼Œæ— æ³•è®¡ç®—æ½œåŠ›æ ‡ç­¾ã€‚")

    # 3. çˆ†æ¬¾é¢„è­¦ï¼ˆæ’­æ”¾/äº’åŠ¨å¼‚å¸¸å¢é•¿ï¼‰
    st.subheader("çˆ†æ¬¾å†…å®¹é¢„è­¦")
    
    # ä¿®æ­£ï¼šå¤„ç†æ ‡å‡†å·®ä¸º0çš„æƒ…å†µï¼Œé¿å… 'float' object has no attribute 'replace' é”™è¯¯
    views_std = df['views'].std()
    engage_std = df['engagement'].std()

    # å¦‚æœæ ‡å‡†å·®ä¸º0ï¼ˆæ‰€æœ‰å€¼éƒ½ä¸€æ ·ï¼‰ï¼ŒZ-scoreä¹Ÿä¸º0ï¼Œå¦åˆ™æ­£å¸¸è®¡ç®—
    df['views_zscore'] = ((df['views'] - df['views'].mean()) / views_std) if views_std > 0 else 0
    df['engage_zscore'] = ((df['engagement'] - df['engagement'].mean()) / engage_std) if engage_std > 0 else 0
    
    hot_df = df[(df['views_zscore'] > 2) | (df['engage_zscore'] > 2)].sort_values('views', ascending=False).head(10)
    st.dataframe(hot_df[['title', 'author', 'views', 'engagement', 'created_at']])


# --- æ ‡ç­¾ç«äº‰åº¦ä¸é•¿å°¾æ ‡ç­¾ ---
def tag_competition_analysis(df):
    st.title("ğŸ·ï¸ æ ‡ç­¾ç«äº‰åº¦ä¸é•¿å°¾æ ‡ç­¾æ¨è")
    st.info("åˆ†ææ ‡ç­¾çš„æµé‡æ½œåŠ›å’Œç«äº‰åº¦ï¼Œæ¨èé«˜è½¬åŒ–é•¿å°¾æ ‡ç­¾ã€‚")
    tag_stats = df.explode('tags_list').groupby('tags_list').agg(
        count=('title', 'count'),
        avg_views=('views', 'mean'),
        avg_engage=('engagement', 'mean')
    )
    tag_stats['ç«äº‰åº¦'] = tag_stats['count'] / tag_stats['avg_views']
    tag_stats['æµé‡æ½œåŠ›'] = tag_stats['avg_views'] * tag_stats['avg_engage']
    st.dataframe(tag_stats.sort_values('æµé‡æ½œåŠ›', ascending=False).head(20))
    st.subheader("é•¿å°¾æ ‡ç­¾æ¨è")
    long_tail = tag_stats[(tag_stats['count'] < tag_stats['count'].median()) & (tag_stats['æµé‡æ½œåŠ›'] > tag_stats['æµé‡æ½œåŠ›'].median())]
    st.dataframe(long_tail.sort_values('æµé‡æ½œåŠ›', ascending=False).head(10))


# --- ç«å“å†…å®¹æ—¥å† ---
def competitor_calendar(df):
    st.title("ğŸ“… ç«å“å†…å®¹æ—¥å†")
    st.info("å¯è§†åŒ–ç«å“è´¦å·çš„å†…å®¹å‘å¸ƒæ—¶é—´å’Œä¸»é¢˜åˆ†å¸ƒï¼Œè¾…åŠ©ä¼˜åŒ–è‡ªèº«å†…å®¹ç­–ç•¥ã€‚")
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œé˜²æ­¢å›¾åƒä¸­æ–‡ä¹±ç 
    set_chinese_font()

    if 'author' not in df.columns or df['author'].nunique() == 0:
        st.warning("æ•°æ®ä¸­æ²¡æœ‰åˆ›ä½œè€…ä¿¡æ¯ï¼Œæ— æ³•è¿›è¡Œæ­¤åˆ†æã€‚")
        return

    # æ ¸å¿ƒæ”¹è¿›ï¼šé¢„å…ˆç­›é€‰å‡ºæœ‰/æ— æœ‰æ•ˆå‘å¸ƒæ—¶é—´çš„åˆ›ä½œè€…
    authors_with_time_data = df.dropna(subset=['created_at'])['author'].unique()

    if len(authors_with_time_data) == 0:
        st.warning("æ‰€æœ‰åˆ›ä½œè€…çš„æ•°æ®éƒ½ç¼ºå°‘æœ‰æ•ˆçš„å‘å¸ƒæ—¶é—´ï¼Œæ— æ³•ç”Ÿæˆä»»ä½•å†…å®¹æ—¥å†ã€‚")
        st.info("è¯·æ£€æŸ¥åŸå§‹æ•°æ®æ–‡ä»¶ä¸­çš„ 'upload_time' åˆ—æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®ï¼ˆä¾‹å¦‚ '2023-10-26 18:00:00'ï¼‰ã€‚")
        return

    all_authors_in_df = df['author'].unique()
    authors_without_time_data = [author for author in all_authors_in_df if author not in authors_with_time_data]

    # è®©ç”¨æˆ·ä»æœ‰æ•°æ®çš„ä½œè€…åˆ—è¡¨ä¸­é€‰æ‹©
    author = st.selectbox("é€‰æ‹©è¦åˆ†æçš„ç«å“è´¦å·ï¼š", authors_with_time_data)

    # å¦‚æœæœ‰ä½œè€…å› ä¸ºæ•°æ®ç¼ºå¤±è€Œæœªæ˜¾ç¤ºï¼Œåˆ™ä»¥å¯å±•å¼€çš„æ–¹å¼å‘ŠçŸ¥ç”¨æˆ·
    if authors_without_time_data:
        with st.expander(f"â„¹ï¸ æ³¨æ„ï¼šæœ‰ {len(authors_without_time_data)} ä½åˆ›ä½œè€…å› ç¼ºå°‘å‘å¸ƒæ—¶é—´æ•°æ®æœªåœ¨åˆ—è¡¨ä¸­æ˜¾ç¤º"):
            st.write("ä»¥ä¸‹åˆ›ä½œè€…æ— æ³•è¿›è¡Œå‘å¸ƒæ—¶é—´åˆ†æï¼Œå› ä¸ºåœ¨æ•°æ®æºä¸­æ‰¾ä¸åˆ°ä»–ä»¬è§†é¢‘çš„æœ‰æ•ˆå‘å¸ƒæ—¥æœŸï¼š")
            # æ˜¾ç¤ºéƒ¨åˆ†åˆ—è¡¨ä»¥é˜²è¿‡é•¿
            st.json(authors_without_time_data[:20])

    # ä½¿ç”¨ .copy() é¿å… SettingWithCopyWarning
    author_df = df[df['author'] == author].copy()

    # æ­¤æ—¶ author_df å¿…ç„¶æœ‰æœ‰æ•ˆçš„ 'created_at'ï¼Œæ— éœ€å†æ¬¡æ£€æŸ¥
    author_df['date'] = author_df['created_at'].dt.date
    author_df['hour'] = author_df['created_at'].dt.hour

    # ä½¿ç”¨ pivot_table åˆ›å»ºçƒ­åŠ›å›¾æ•°æ®
    pivot = author_df.pivot_table(index='date', columns='hour', values='views', aggfunc='sum', fill_value=0)

    if pivot.empty:
        st.info(f"åˆ›ä½œè€… {author} çš„æ•°æ®æœ‰æ•ˆï¼Œä½†å¯èƒ½ä¸è¶³ä»¥ç”Ÿæˆçƒ­åŠ›å›¾ã€‚")
        return

    # ä¼˜åŒ–çƒ­åŠ›å›¾å¯è§†åŒ–
    fig, ax = plt.subplots(figsize=(15, max(5, len(pivot.index) // 2)))
    sns.heatmap(pivot, cmap='YlGnBu', ax=ax, linewidths=.5, annot=True, fmt=".0f", annot_kws={"size": 8})
    ax.set_title(f"{author} å†…å®¹å‘å¸ƒçƒ­åŠ›å›¾ (æŒ‰å°æ—¶æ’­æ”¾é‡æ€»å’Œ)", fontsize=14)
    ax.set_xlabel("å°æ—¶ (24H)", fontsize=12)
    ax.set_ylabel("æ—¥æœŸ", fontsize=12)
    plt.xticks(rotation=0)
    plt.yticks(rotation=0)
    st.pyplot(fig)

    st.dataframe(author_df[['title', 'tags_list', 'created_at', 'views', 'engagement']].sort_values('created_at', ascending=False))


# --- æŠ¥å‘Šè‡ªåŠ¨æ´å¯Ÿ ---
def generate_insight(df):
    """åŸºäºè§„åˆ™å’Œæ•°æ®å½’çº³è‡ªåŠ¨ç”Ÿæˆæœ¬æœŸå†…å®¹æ´å¯Ÿ"""
    total_videos = len(df)
    top_tag = df.explode('tags_list')['tags_list'].value_counts().idxmax()
    top_author = df['author'].value_counts().idxmax()
    avg_views = df['views'].mean()
    avg_engage = df['engagement'].mean()
    insight = f"æœ¬æœŸå…±åˆ†æ{total_videos}æ¡è§†é¢‘ï¼Œæœ€çƒ­é—¨æ ‡ç­¾ä¸ºã€{top_tag}ã€‘ï¼Œå¤´éƒ¨åˆ›ä½œè€…ä¸ºã€{top_author}ã€‘ã€‚å¹³å‡æ’­æ”¾é‡{avg_views:,.0f}ï¼Œå¹³å‡äº’åŠ¨é‡{avg_engage:,.0f}ã€‚å»ºè®®å…³æ³¨é«˜æ½œåŠ›æ ‡ç­¾å’Œå¤´éƒ¨è´¦å·çš„å†…å®¹ç­–ç•¥ï¼Œç»“åˆè¶‹åŠ¿é¢„æµ‹å’Œæ ‡ç­¾ç«äº‰åº¦åˆ†æä¼˜åŒ–é€‰é¢˜å’Œæ ‡ç­¾ç»„åˆã€‚"
    return insight


# ==============================================================================
# 5. ä¸»ç¨‹åºå…¥å£
# ==============================================================================
def get_data_file_path():
    """
    åŠ¨æ€æŸ¥æ‰¾æ•°æ®æ–‡ä»¶ 'bilibili_hot_with_tags.csv'ã€‚
    ä¼˜å…ˆåœ¨è„šæœ¬æ‰€åœ¨ç›®å½•æŸ¥æ‰¾ï¼Œå…¶æ¬¡åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼ˆä¸Šä¸€çº§ï¼‰æŸ¥æ‰¾ã€‚
    """
    script_path = Path(__file__).resolve()
    # å®šä¹‰å¯èƒ½çš„è·¯å¾„
    current_dir_path = script_path.parent / "bilibili_hot_with_tags.csv"
    parent_dir_path = script_path.parent.parent / "bilibili_hot_with_tags.csv"

    if current_dir_path.exists():
        st.sidebar.info(f"å·²ä»å½“å‰ç›®å½•åŠ è½½æ•°æ®ã€‚")
        return current_dir_path
    elif parent_dir_path.exists():
        st.sidebar.info(f"å·²ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½æ•°æ®ã€‚")
        return parent_dir_path
    else:
        st.sidebar.error("é”™è¯¯ï¼šæœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ 'bilibili_hot_with_tags.csv'ã€‚")
        st.sidebar.info("è¯·å°†æ•°æ®æ–‡ä»¶æ”¾ç½®åœ¨ä¸è„šæœ¬ç›¸åŒçš„ 'ä¸“ä¸šå®ä¹ ' ç›®å½•ï¼Œæˆ–é¡¹ç›®çš„æ ¹ç›®å½•ä¸­ã€‚")
        return None


def main():
    """ä¸»åº”ç”¨æ§åˆ¶å™¨ã€‚"""
    st.sidebar.title("Bç«™å†…å®¹å†³ç­–å·¥å…· v3.0")
    
    # ä½¿ç”¨å¥å£®çš„æ–¹å¼è·å–æ•°æ®æ–‡ä»¶è·¯å¾„
    data_path = get_data_file_path()
    if data_path is None:
        # å¦‚æœæ‰¾ä¸åˆ°æ–‡ä»¶ï¼Œåˆ™åœæ­¢æ‰§è¡Œ
        return

    df = load_data(data_path)
    
    # å¢åŠ æ•°æ®åŠ è½½æ ¡éªŒ
    if df is None or df.empty:
        st.error("æ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶å†…å®¹æˆ–æ ¼å¼ã€‚")
        return

    st.sidebar.title("å¯¼èˆªèœå•")
    page_options = {
        "ğŸ“ˆ çƒ­ç‚¹è¶‹åŠ¿æ€»è§ˆ": page_overview,
        "ğŸ“Š è¶‹åŠ¿é¢„æµ‹": page_trend_forecast,
        "ğŸ·ï¸ æ ‡ç­¾ç«äº‰åº¦": tag_competition_analysis,
        "ğŸ“… ç«å“å†…å®¹æ—¥å†": competitor_calendar,
        "ğŸ”‘ çˆ†æ¬¾å¯†ç æŒ–æ˜": page_association_rules,
        "ğŸ¯ å†…å®¹ç­–ç•¥åˆ†æ": page_strategy_analysis,
        "ğŸ¤– AIæ¨¡å‹ä¸­å¿ƒ": page_models_center,
        "ğŸ“„ æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆ": page_report_generation
    }
    selection = st.sidebar.radio("é€‰æ‹©åˆ†ææ¨¡å—", list(page_options.keys()))
    page_func = page_options[selection]
    page_func(df)


if __name__ == "__main__":
    main()